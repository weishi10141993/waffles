{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import waffles.input_output.raw_hdf5_reader as reader\n",
    "import waffles.np04_analysis.led_calibration.utils as led_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_run_to_pickled_WaveformSet(\n",
    "    run: int,\n",
    "    saving_folderpath: str,\n",
    "    average_wfs_per_channel: int = 4000,\n",
    "    channels: Optional[dict] = None,\n",
    "    channels_no: int = 40,\n",
    "    rucio_filepaths_folderpath: str = \"/eos/experiment/neutplatform/protodune/experiments/ProtoDUNE-II/PDS_Commissioning/waffles/1_rucio_paths/\",\n",
    "    read_full_streaming_data: bool = False,\n",
    "    self_trigger_endpoints: list[int] = [109, 111, 112, 113],\n",
    "    full_streaming_endpoints: list[int] = [104, 105, 107],\n",
    "    subsample_seed: int = 3,\n",
    "    verbose: bool = True\n",
    "    ) -> None:\n",
    "    \"\"\"This function gets the following positional arguments:\n",
    "\n",
    "    - run (int): Number of the run whose data we want to convert\n",
    "    to a pickle'd WaveformSet.\n",
    "    - saving_folderpath (str): Path to the folder where to save\n",
    "    the pickle'd WaveformSet(s).\n",
    "\n",
    "    This function gets the following keyword arguments:\n",
    "\n",
    "    - average_wfs_per_channel (int): Assuming that the read data\n",
    "    is homogeneously distributed along the detector channels,\n",
    "    the pickle'd WaveformSet object(s) will contain, on average,\n",
    "    average_wfs_per_channel Waveform objects per detector\n",
    "    channel.\n",
    "    - channels (dictionary): A dictionary with the channels\n",
    "    to be read for this run. The keys of the dictionary\n",
    "    are the endpoint values, while the values are the\n",
    "    channel numbers. If this parameter is defined, then\n",
    "    the channels_no parameter is ignored, and the channels\n",
    "    number is computed as the number of different channels\n",
    "    in this dictionary. However, note that prior to this\n",
    "    counting, the endpoints in this dictionary are filtered\n",
    "    according to the following three parameters:\n",
    "    read_full_streaming_data, self_trigger_endpoints and\n",
    "    full_streaming_endpoints. The filtered channels parameter\n",
    "    is given to the 'ch' parameter of the\n",
    "    WaveformSet_from_hdf5_file() function. Check such function\n",
    "    docstring for more information.\n",
    "    - channels_no (int): Number of channels in the detector.\n",
    "    This parameter makes a difference only if the 'channels'\n",
    "    parameter is not defined.\n",
    "    - rucio_filepaths_folderpath (str): Path to the folder\n",
    "    where the files with the rucio filepaths are stored.\n",
    "    The file which contains the rucio filepaths for a\n",
    "    given run number, <run>, is assumed to be called \n",
    "    '0<run>.txt',\n",
    "    - read_full_streaming_data (bool): Whether to read the\n",
    "    full-streaming data of the self-trigger data.\n",
    "    - self_trigger_endpoints (resp. full_streaming_endpoints)\n",
    "    (list of integers): The list of endpoints which contain\n",
    "    self-triggered (resp. full-streaming) channels. This\n",
    "    parameter makes a difference only if the 'channels'\n",
    "    parameter is defined. Indeed, the\n",
    "    WaveformSet_from_hdf5_file() function knows per se which\n",
    "    endpoints/channels are self-triggered or full-streamed.\n",
    "    However, if the 'channels' parameter is defined, we need\n",
    "    to know this information prior to calling\n",
    "    WaveformSet_from_hdf5_file() so that we can compute the\n",
    "    actual number of channels which we are targeting, and\n",
    "    so, the number of waveforms that we need to read in\n",
    "    order to get an average of average_wfs_per_channel\n",
    "    waveforms per channel.\n",
    "    - subsample_seed (int): The seed for the subsample\n",
    "    parameter. This parameter is decreased unit by unit\n",
    "    until the number of Waveform objects in the resulting\n",
    "    WaveformSet object reaches\n",
    "    average_wfs_per_channel * number_of_channels, where\n",
    "    number_of_channels is the number of different channels\n",
    "    read for this run. This parameter is given to the\n",
    "    'subsample' parameter of the\n",
    "    WaveformSet_from_hdf5_file() function. Check such\n",
    "    function docstring for more information.\n",
    "    - verbose (bool): Whether to print functioning-related\n",
    "    messages.\n",
    "    \n",
    "    This function looks for a file called '0<run>.txt' within\n",
    "    the folder whose path is given by \n",
    "    rucio_filepaths_folderpath. Such file is assumed to be a\n",
    "    text file with a list of filepaths. Parsing such file is\n",
    "    delegated to the get_filepaths_from_rucio() function of\n",
    "    'raw_hdf5_reader.py' module. If it is found, then \n",
    "    it starts reading WaveformSet(s), one per filepath, until\n",
    "    the total number of read waveforms has reached\n",
    "    average_wfs_per_channel * number_of_channels waveforms,\n",
    "    where number_of_channels is the number of different\n",
    "    channels read for this run. Reading the WaveformSet(s)\n",
    "    is delegated to the WaveformSet_from_hdf5_file() function\n",
    "    of the 'raw_hdf5_reader.py' module. The read WaveformSet(s)\n",
    "    are pickle'd to files which are saved in the folder pointed \n",
    "    to by saving_folderpath. The WaveformSet coming from \n",
    "    the i-th filepath is saved to the file named\n",
    "    'run_<run>_chunk_<i>_FS.pkl' if read_full_streaming_data is\n",
    "    True, and 'run_<run>_chunk_<i>_ST.pkl' otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    if channels is None:\n",
    "        fChannelsAreDefined = False\n",
    "        number_of_channels = channels_no\n",
    "\n",
    "    else:\n",
    "        # In this case, compute the number of different\n",
    "        # channels making sure that there are no duplicates\n",
    "        fChannelsAreDefined = True\n",
    "        number_of_channels = 0\n",
    "        channels_ = {}\n",
    "\n",
    "        # Filter the given 'channels' parameter to:\n",
    "        #   - erase endpoints with no targeted channels\n",
    "        #   - erase endpoints whose type (full-streaming\n",
    "        #     or self-trigger) do not match the \n",
    "        #     read_full_streaming_data parameter\n",
    "        #   - erase duplicated channels\n",
    "        for endpoint in channels.keys():\n",
    "            if len(channels[endpoint]) > 0:\n",
    "                if read_full_streaming_data:\n",
    "                    if endpoint in full_streaming_endpoints:\n",
    "                        channels_[endpoint] = list(set(channels[endpoint]))\n",
    "                        number_of_channels += len(channels_[endpoint])\n",
    "                else:\n",
    "                    if endpoint in self_trigger_endpoints:\n",
    "                        channels_[endpoint] = list(set(channels[endpoint]))\n",
    "                        number_of_channels += len(channels_[endpoint])\n",
    "\n",
    "        if len(channels_) == 0:\n",
    "            raise Exception(\n",
    "                \"In function save_run_to_pickled_WaveformSet(): \"\n",
    "                f\"After filtering the given channels parameter ({channels}) \"\n",
    "                \"no channels were left. Make sure that the type of data \"\n",
    "                \"which you are requiring (read_full_streaming_data=\"\n",
    "                f\"{read_full_streaming_data}) matches the specified run \"\n",
    "                f\"({run}).\"\n",
    "            )\n",
    "\n",
    "    aux = os.path.join(\n",
    "        rucio_filepaths_folderpath,\n",
    "        f\"0{run}.txt\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        rucio_filepaths = reader.get_filepaths_from_rucio(aux)\n",
    "    # Happens if there are no rucio filepaths for this run in rucio_filepaths_folderpath\n",
    "    except Exception:\n",
    "        print(\n",
    "            f\"In function save_run_to_pickled_WaveformSet(): The rucio \"\n",
    "            f\"paths for run {run} were not found. Ending execution of \"\n",
    "            f\"save_run_to_pickled_WaveformSet({run}, ...).\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    chunks_no = len(rucio_filepaths)\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"In function save_run_to_pickled_WaveformSet(): \"\n",
    "            f\"Processing run {run} - found {chunks_no} chunk(s) ...\"\n",
    "        )\n",
    "\n",
    "    fGoForAnotherChunk = True\n",
    "    wvfs_left_to_read_for_this_run = average_wfs_per_channel * number_of_channels\n",
    "    current_chunk_iterator = 0\n",
    "\n",
    "    while fGoForAnotherChunk:\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\\t --> Processing chunk {current_chunk_iterator+1}\"\n",
    "                f\"/{chunks_no} ...\"\n",
    "            )\n",
    "\n",
    "        subsample = subsample_seed\n",
    "        fReadSameChunkAgain = True\n",
    "\n",
    "        while fReadSameChunkAgain:\n",
    "\n",
    "            aux_wfset = reader.WaveformSet_from_hdf5_file(\n",
    "                rucio_filepaths[current_chunk_iterator],\n",
    "                read_full_streaming_data=read_full_streaming_data,\n",
    "                subsample=subsample,\n",
    "                # WaveformSet_from_hdf5_file apparently subsamples from\n",
    "                # the [0, wvfm_count] range. Therefore, if we set\n",
    "                # wvfm_count to wvfs_left_to_read_for_this_run we\n",
    "                # will get, at most, wvfs_left_to_read_for_this_run/subsample\n",
    "                wvfm_count=wvfs_left_to_read_for_this_run*subsample,\n",
    "                # WaveformSet_from_hdf5_file ignores the 'ch' parameter\n",
    "                # if it is an empty dictionary\n",
    "                ch=channels_ if fChannelsAreDefined else {}\n",
    "            )\n",
    "            \n",
    "            # In this case, we already have what we need for this run\n",
    "            if len(aux_wfset.waveforms) == wvfs_left_to_read_for_this_run:\n",
    "                fReadSameChunkAgain = False\n",
    "                fGoForAnotherChunk = False\n",
    "\n",
    "                aux = f\"run_{run}_chunk_{current_chunk_iterator}_awpc_\"+\\\n",
    "                    f\"{round(len(aux_wfset.waveforms)/number_of_channels)}\"\n",
    "                aux_saving_filename = f\"{aux}_FS.pkl\" if read_full_streaming_data else f\"{aux}_ST.pkl\"\n",
    "\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        \"\\t\\t --> Got enough waveforms (\"\n",
    "                        f\"{len(aux_wfset.waveforms)}) from chunk \"\n",
    "                        f\"{current_chunk_iterator+1}/{chunks_no} \"\n",
    "                        f\"of run {run}\"\n",
    "                    )\n",
    "                    print(\n",
    "                        \"\\t --> Now saving it to a pickle file in \"\n",
    "                        f\"{os.path.join(saving_folderpath, aux_saving_filename)} ...\"\n",
    "                    )\n",
    "\n",
    "                led_utils.dump_object_to_pickle(\n",
    "                    aux_wfset,\n",
    "                    saving_folderpath,\n",
    "                    aux_saving_filename,\n",
    "                    verbose=verbose\n",
    "                )\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\n",
    "                        \"In function save_run_to_pickled_WaveformSet(): \"\n",
    "                        f\"Successfully saved WaveformSet of run {run}\"\n",
    "                    )\n",
    "\n",
    "            # In this case, we need more waveforms for this run\n",
    "            elif len(aux_wfset.waveforms) < wvfs_left_to_read_for_this_run:\n",
    "\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"\\t\\t --> Didn't get enough waveforms from chunk \"\n",
    "                        f\"{current_chunk_iterator+1}/{chunks_no} of run {run}\"\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"\\t\\t --> Expected {wvfs_left_to_read_for_this_run}, \"\n",
    "                        f\"but only read {len(aux_wfset.waveforms)}\"\n",
    "                    )\n",
    "\n",
    "                # In this case, try to read the same file but with a finer subsampling\n",
    "                if subsample > 1:\n",
    "                    # fReadSameChunkAgain is True by default\n",
    "                    previous_subsample = subsample\n",
    "                    subsample = led_utils.next_subsample(\n",
    "                        subsample, # current_subsample\n",
    "                        len(aux_wfset.waveforms), # read_quantity\n",
    "                        wvfs_left_to_read_for_this_run # required_quantity\n",
    "                    )\n",
    "\n",
    "                    if verbose:\n",
    "                        print(\n",
    "                            f\"\\t\\t --> Switching 'subsample' from {previous_subsample} \"\n",
    "                            f\"to {subsample} and reading it again ...\"\n",
    "                        )\n",
    "\n",
    "                # In this case, we read every waveform from this chunk, but we still\n",
    "                # haven't got enough waveforms, so go for the following chunk\n",
    "                else:\n",
    "                    subsample = subsample_seed\n",
    "                    fReadSameChunkAgain = False\n",
    "                    # fGoForAnotherChunk is True by default\n",
    "\n",
    "                    aux = f\"run_{run}_chunk_{current_chunk_iterator}_awpc_\"+\\\n",
    "                        f\"{round(len(aux_wfset.waveforms)/number_of_channels)}\"\n",
    "                    aux_saving_filename = f\"{aux}_FS.pkl\" if read_full_streaming_data else f\"{aux}_ST.pkl\"\n",
    "\n",
    "                    if verbose:\n",
    "                        print(\n",
    "                            f\"\\t\\t --> All of the waveforms from this chunk were read\"\n",
    "                        )\n",
    "                        print(\n",
    "                            \"\\t --> Saving them to a pickle file in \"\n",
    "                            f\"{os.path.join(saving_folderpath, aux_saving_filename)} ...\"\n",
    "                        )\n",
    "\n",
    "                    led_utils.dump_object_to_pickle(\n",
    "                        aux_wfset,\n",
    "                        saving_folderpath,\n",
    "                        aux_saving_filename,\n",
    "                        verbose=verbose\n",
    "                    )\n",
    "\n",
    "                    # Switch to next chunk if there's another chunk\n",
    "                    current_chunk_iterator += 1\n",
    "                    if current_chunk_iterator >= chunks_no:\n",
    "                        if verbose:\n",
    "                            print(\n",
    "                                f\"\\t\\t --> There are no more chunks available for this run. \"\n",
    "                                \"Although the targeted number-of-waveforms per channel was \"\n",
    "                                \"not achieved for this run, the available ones have been saved. \"\n",
    "                            )\n",
    "\n",
    "                        fGoForAnotherChunk = False\n",
    "\n",
    "                    else:\n",
    "                        if verbose:\n",
    "                            print(\n",
    "                                f\"\\t\\t --> Proceeding to look for \"\n",
    "                                f\"{wvfs_left_to_read_for_this_run-len(aux_wfset.waveforms)} \"\n",
    "                                f\"(={wvfs_left_to_read_for_this_run}-{len(aux_wfset.waveforms)}) \"\n",
    "                                f\"waveforms from the following chunk ({current_chunk_iterator+1}/\"\n",
    "                                f\"{chunks_no}) of this run ({run}).\"\n",
    "                            )\n",
    "                        \n",
    "                    # But only read the waveforms that we need to add up to \n",
    "                    # average_wfs_per_channel * number_of_channels\n",
    "                    wvfs_left_to_read_for_this_run -= len(aux_wfset.waveforms)\n",
    "                    \n",
    "            # In this case, WaveformSet_from_hdf5_file() is misbehaving\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    \"In function save_run_to_pickled_WaveformSet(): \"\n",
    "                    f\"WaveformSet_from_hdf5_file() is misbehaving. It read\"\n",
    "                    f\" more waveforms ({len(aux_wfset.waveforms)}) than \"\n",
    "                    f\"specified (wvfm_count={average_wfs_per_channel * number_of_channels})\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of batches to target, p.e. [6, 7]\n",
    "batch_nos = [6, 7]\n",
    "for _ in batch_nos:\n",
    "    if _ not in range(1, 8):\n",
    "        raise Exception(\n",
    "            f\"Found batch number {_}. All the requested \"\n",
    "            \"batch numbers must be in the range [1, 7].\"\n",
    "        )\n",
    "\n",
    "# List of PDEs to target, p.e. [40, 45]\n",
    "pdes = [45, 50]\n",
    "for _ in pdes:\n",
    "    if _ not in [40., 45., 50.]:\n",
    "        raise Exception(\n",
    "            f\"Found PDE {_}. All requested PDEs must be \"\n",
    "            \"one of the following values: [40., 45., 50.]\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the channels-per-run database of the LED-calibration (typically placed in src/waffles/np04_analysis/led_calibration/scripts/batch_pickle_generator.ipynb)\n",
    "channels_per_run_database_filepath = \"\"\n",
    "# The folder containing TXT files with the 0<run>.txt format with the needed rucio paths\n",
    "base_rucio_filepaths_folderpath = \"\"\n",
    "# The folder where the pickle'd WaveformSets will be saved\n",
    "base_saving_folderpath = \"\"\n",
    "# Average number of waveforms per channel\n",
    "average_wfs_per_channel = 3000\n",
    "# The first value to try for the 'subsample' parameter of WaveformSet_from_hdf5_file()\n",
    "subsample_seed = 3\n",
    "# Whether to print functioning related messages\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data into pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(channels_per_run_database_filepath)\n",
    "\n",
    "for batch_no in batch_nos:\n",
    "    for pde in pdes:\n",
    "\n",
    "        # Filter out the runs which do not belong to the current\n",
    "        # batch and PDE\n",
    "        filtered_df = df[\n",
    "            (df['batch'] == batch_no) &\n",
    "            (df['pde'] == pde/100.)\n",
    "        ]\n",
    "\n",
    "        # Tuple of run numbers to read for the current batch and PDE\n",
    "        runs = tuple(filtered_df['run'].tolist())\n",
    "\n",
    "        # acquired_apas[i] is the list of acquired-APAs numbers\n",
    "        acquired_apas = tuple([\n",
    "            led_utils.parse_numeric_list(x)\n",
    "            for x in filtered_df['acquired_apas'].tolist()\n",
    "        ])\n",
    "\n",
    "        # The folder where to look for the rucio paths for this \n",
    "        # batch and PDE\n",
    "        rucio_filepaths_folderpath_candidate = os.path.join(\n",
    "            base_rucio_filepaths_folderpath,\n",
    "            f\"batch_{batch_no}/pde_{pde}\"\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(rucio_filepaths_folderpath_candidate):\n",
    "            print(\n",
    "                f\"WARNING: The folder {rucio_filepaths_folderpath_candidate} \"\n",
    "                \"does not exist. Skipping the data download for batch \"\n",
    "                f\"{batch_no} and PDE {pde}.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        for i, run in enumerate(runs):\n",
    "\n",
    "            # Both flags may be simultaneously True, if a during a certain run\n",
    "            # APA 1 and another APA were acquired at the same time. P.e. run 27898\n",
    "            fReadFullStreamingData = False\n",
    "            fReadSelfTriggerData = False\n",
    "\n",
    "            if 1 in acquired_apas[i]:\n",
    "                fReadFullStreamingData = True\n",
    "\n",
    "            if 2 in acquired_apas[i] or 3 in acquired_apas[i] or 4 in acquired_apas[i]:\n",
    "                fReadSelfTriggerData = True\n",
    "\n",
    "            aux = df[\n",
    "                df['run'] == run\n",
    "            ].iloc[0]['aimed_channels']\n",
    "\n",
    "            if aux == '[]':\n",
    "                print(\n",
    "                    f\"WARNING: The run {run} does not have any \"\n",
    "                    \"aimed channels defined in the database. Skipping \"\n",
    "                    f\"this run.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # This is a dictionary of channels with the following format:\n",
    "            # {112: [35, 37, 21, 23, 25], 113: [0], 111: [11, 44, 45]}\n",
    "            formatted_channels = led_utils.arrange_dictionary_of_endpoints_and_channels(\n",
    "                led_utils.parse_numeric_list(aux)\n",
    "            )\n",
    "            \n",
    "            # The folder where the created pickles will be saved\n",
    "            saving_folderpath = os.path.join(\n",
    "                base_saving_folderpath,\n",
    "                f\"{average_wfs_per_channel}_wfs_per_channel\"\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(saving_folderpath):\n",
    "                os.makedirs(saving_folderpath)\n",
    "\n",
    "            # OPEN ISSUE: Doing several tests I noted that in the data-taking for APA 1\n",
    "            # in batches 4 and 5, the WaveformSet_from_hdf5_file() function does not seem\n",
    "            # to find data for the following channels {105: [0, 2, 3], 104: [10, 13, 14,\n",
    "            # 15, 17], 107: [10, 12]} (it does find data for the channels {105: {24, 5},\n",
    "            # 104: {16, 11, 12}}} though). The affected runs are 28634 (batch 4) and 28980\n",
    "            # (batch 5). As a side effect, this code saves thrice as waveforms for the\n",
    "            # found channels, which is not a big deal (it saves a fixed number of waveforms\n",
    "            # among a number of channels which was expected to be thrice as big - 5 vs 15).\n",
    "            # However, the reason why there is no data available for those channels in the\n",
    "            # raw HDF5 files should be investigated.\n",
    "\n",
    "            rucio_filepaths_folderpath=os.path.join(\n",
    "                base_rucio_filepaths_folderpath,\n",
    "                f\"batch_{batch_no}/pde_{pde}\"\n",
    "            )\n",
    "\n",
    "            if fReadFullStreamingData:\n",
    "                save_run_to_pickled_WaveformSet(\n",
    "                    run,\n",
    "                    saving_folderpath,\n",
    "                    average_wfs_per_channel=average_wfs_per_channel,\n",
    "                    # For an acquired_apas=[1,2] case, we'll be giving APAs 1&2 channels\n",
    "                    # in formatted_channels. Note, however, that\n",
    "                    # reader.WaveformSet_from_hdf5_file will correctly identify which\n",
    "                    # channels should be read thanks to the read_full_streaming_data\n",
    "                    # parameter\n",
    "                    channels=formatted_channels,\n",
    "                    rucio_filepaths_folderpath=rucio_filepaths_folderpath,\n",
    "                    read_full_streaming_data=True,\n",
    "                    subsample_seed=subsample_seed,\n",
    "                    verbose=verbose\n",
    "                )\n",
    "\n",
    "            if fReadSelfTriggerData:\n",
    "                save_run_to_pickled_WaveformSet(\n",
    "                    run,\n",
    "                    saving_folderpath,\n",
    "                    average_wfs_per_channel=average_wfs_per_channel,\n",
    "                    channels=formatted_channels,\n",
    "                    rucio_filepaths_folderpath=rucio_filepaths_folderpath,\n",
    "                    read_full_streaming_data=False,\n",
    "                    subsample_seed=subsample_seed,\n",
    "                    verbose=verbose\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Code to test the number of retrieved waveforms, channel by channel\n",
    "# from waffles.input_output.pickle_file_reader import WaveformSet_from_pickle_file\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# # The following 5 parameters must be set mutually\n",
    "# batch = 4\n",
    "# pde = 45\n",
    "# run = 28634\n",
    "# read_full_streaming_data = True\n",
    "# channels = led_utils.arrange_dictionary_of_endpoints_and_channels([10500, 10502, 10503, 10505, 10410, 10411, 10412, 10413, 10414, 10415, 10416, 10417, 10710, 10712, 10524])\n",
    "\n",
    "# save_run_to_pickled_WaveformSet(\n",
    "#     run,\n",
    "#     \"\", # Path where to save the pickle\n",
    "#     average_wfs_per_channel=200,\n",
    "#     channels=channels,\n",
    "#     rucio_filepaths_folderpath=\"\", # Path where to look for the rucio paths\n",
    "#     read_full_streaming_data=read_full_streaming_data,\n",
    "#     subsample_seed=subsample_seed,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# mywfset = WaveformSet_from_pickle_file(\n",
    "#     \"\" # Copy here the output filepath prompted by the call to save_run_to_pickled_WaveformSet() above\n",
    "# )\n",
    "\n",
    "# print(\"---------------- Expected channels ----------------\")\n",
    "# print(channels)\n",
    "# print(\"---------------- Found channels ----------------\")\n",
    "# print(mywfset.available_channels)\n",
    "\n",
    "# print(\"---------------- number-of-waveorms distribution through channels ----------------\")\n",
    "# samples = []\n",
    "# for endpoint in mywfset.available_channels[run].keys():\n",
    "#     for channel in mywfset.available_channels[run][endpoint]:\n",
    "\n",
    "#         count = 0\n",
    "#         for wf in mywfset.waveforms:\n",
    "#             if wf.endpoint == endpoint and wf.channel == channel:\n",
    "#                 count += 1\n",
    "#         print(f\"Found {count} waveforms with endpoint {endpoint} and channel {channel} in the WaveformSet.\")\n",
    "#         samples.append(count)\n",
    "\n",
    "# plt.hist(samples, bins=20)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
