{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import waffles.input_output.raw_hdf5_reader as reader\n",
    "from waffles.np04_analysis.led_calibration.configs.calibration_batches.run_number_to_LED_configuration import run_to_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some convenient definitions\n",
    "\n",
    "def dump_object_to_pickle(\n",
    "        object, \n",
    "        saving_folderpath : str,\n",
    "        output_filename : str,\n",
    "        verbose : bool = True) -> None:\n",
    "    \"\"\"This function gets the following positional argument:\n",
    "\n",
    "    - object\n",
    "    - saving_folderpath (str): Path to the folder\n",
    "    where to save the file.\n",
    "    - output_filename (str): Name of the output \n",
    "    pickle file.\n",
    "\n",
    "    And the following keyword argument:\n",
    "\n",
    "    - verbose (bool): Whether to print functioning\n",
    "    related messages.\n",
    "    \n",
    "    It saves the given object, object, to a pickle file \n",
    "    which is stored in the path given by saving_filepath\"\"\"\n",
    "\n",
    "    # If the saving folder does not exist, create it\n",
    "    if not os.path.exists(saving_folderpath):\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"In function dump_object_to_pickle(): Folder {saving_folderpath} does not exist. It will be created.\")\n",
    "\n",
    "        os.makedirs(saving_folderpath)\n",
    "\n",
    "    # Create the output filepath\n",
    "    output_filepath = os.path.join(\n",
    "        saving_folderpath, \n",
    "        output_filename)\n",
    "    \n",
    "    with open(\n",
    "        output_filepath, \n",
    "        \"wb\") as output_file:\n",
    "\n",
    "        pickle.dump(object, output_file)\n",
    "\n",
    "        return\n",
    "\n",
    "def save_run_to_pickled_WaveformSet(\n",
    "    run : int,\n",
    "    saving_folderpath : str,\n",
    "    average_wfs_per_channel : int = 4000,\n",
    "    channels_no : int = 40,\n",
    "    rucio_filepaths_folderpath : str = \"/eos/experiment/neutplatform/protodune/experiments/ProtoDUNE-II/PDS_Commissioning/waffles/1_rucio_paths/\",\n",
    "    read_full_streaming_data : bool = False,\n",
    "    subsample_seed : int = 3,\n",
    "    verbose : bool = True):\n",
    "    \"\"\"This function gets the following positional arguments:\n",
    "\n",
    "    - run (int): Number of the run whose data we want to convert\n",
    "    to a pickle'd WaveformSet.\n",
    "    - saving_folderpath (str): Path to the folder where to save\n",
    "    the pickle'd WaveformSet(s).\n",
    "\n",
    "    This function gets the following keyword arguments:\n",
    "\n",
    "    - average_wfs_per_channel (int): Assuming that the read data\n",
    "    is homogeneously distributed along the detector channels,\n",
    "    the pickle'd WaveformSet object(s) will contain, on average,\n",
    "    average_wfs_per_channel Waveform objects per detector\n",
    "    channel.\n",
    "    - channels_no (int): Number of channels in the detector.\n",
    "    - rucio_filepaths_folderpath (str): Path to the folder\n",
    "    where the files with the rucio filepaths are stored.\n",
    "    The file which contains the rucio filepaths for a\n",
    "    given run number, <run>, is assumed to be called \n",
    "    '0<run>.txt',\n",
    "    - read_full_streaming_data (bool): Whether to read the\n",
    "    full-streaming data of the self-trigger data.\n",
    "    - subsample_seed (int): The seed for the subsample\n",
    "    parameter. This parameter is decreased unit by unit\n",
    "    until the number of Waveform objects in the resulting\n",
    "    WaveformSet object reaches\n",
    "    average_wfs_per_channel * channels_no. This parameter\n",
    "    is given to the 'subsample' parameter of the\n",
    "    WaveformSet_from_hdf5_file() function. Check such function\n",
    "    docstring for more information.\n",
    "    - verbose (bool): Whether to print functioning-related\n",
    "    messages.\n",
    "    \n",
    "    This function looks for a file called '0<run>.txt' within\n",
    "    the folder whose path is given by \n",
    "    rucio_filepaths_folderpath. Such file is assumed to be a\n",
    "    text file with a list of filepaths. Parsing such file is\n",
    "    delegated to the get_filepaths_from_rucio() function of\n",
    "    'raw_hdf5_reader.py' module. If it is found, then \n",
    "    it starts reading WaveformSet(s), one per filepath, until\n",
    "    the total number of read waveforms have reached\n",
    "    average_wfs_per_channel * channels_no waveforms. Reading\n",
    "    the WaveformSet(s) is delegated to the\n",
    "    WaveformSet_from_hdf5_file() function of the \n",
    "    'raw_hdf5_reader.py' module. The read WaveformSet(s) are \n",
    "    pickle'd to files which are saved in the folder pointed \n",
    "    to by saving_folderpath. The WaveformSet coming from \n",
    "    the i-th filepath is saved to the file named\n",
    "    'run_<run>_chunk_<i>.pkl'\n",
    "    \"\"\"\n",
    "\n",
    "    aux = rucio_filepaths_folderpath+f\"/0{run}.txt\"\n",
    "\n",
    "    try:\n",
    "        rucio_filepaths = reader.get_filepaths_from_rucio(aux)\n",
    "    # Happens if there are no rucio filepaths for this run in rucio_filepaths_folderpath\n",
    "    except Exception:\n",
    "        print(\n",
    "            f\"--> WARNING: Did not find the rucio paths for run {run}. Ending execution \"\n",
    "            f\"of save_run_to_pickled_WaveformSet({run}, ...).\")\n",
    "        return\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"--> Processing run {run}: Found {len(rucio_filepaths)} chunks...\")\n",
    "\n",
    "    fGoForAnotherChunk = True\n",
    "    wvfs_left_to_read_for_this_run = average_wfs_per_channel * channels_no\n",
    "    current_chunk_iterator = 0\n",
    "\n",
    "    while fGoForAnotherChunk:\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\t --> Processing chunk {current_chunk_iterator+1}/{len(rucio_filepaths)} ...\")\n",
    "\n",
    "        subsample = subsample_seed\n",
    "        fReadSameChunkAgain = True\n",
    "\n",
    "        while fReadSameChunkAgain:\n",
    "\n",
    "            aux_wfset = reader.WaveformSet_from_hdf5_file(\n",
    "                rucio_filepaths[current_chunk_iterator],\n",
    "                read_full_streaming_data=read_full_streaming_data,\n",
    "                subsample=subsample,\n",
    "                # WaveformSet_from_hdf5_file apparently subsamples from\n",
    "                # the [0, wvfm_count] range. Therefore, if we set\n",
    "                # wvfm_count to wvfs_left_to_read_for_this_run we\n",
    "                # will get, at most, wvfs_left_to_read_for_this_run/subsample\n",
    "                wvfm_count=wvfs_left_to_read_for_this_run*subsample,\n",
    "                )\n",
    "            \n",
    "            # In this case, we already have what we need for this run\n",
    "            if len(aux_wfset.waveforms) == wvfs_left_to_read_for_this_run:\n",
    "                fReadSameChunkAgain = False\n",
    "                fGoForAnotherChunk = False\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"--> Got enough waveforms ({len(aux_wfset.waveforms)}) \"\n",
    "                          f\"from chunk {current_chunk_iterator+1}/{len(rucio_filepaths)} \"\n",
    "                          f\"of run {run}\")\n",
    "                    print(f\"--> Now saving it to a pickle file ...\")\n",
    "\n",
    "                dump_object_to_pickle(\n",
    "                    aux_wfset,\n",
    "                    saving_folderpath,\n",
    "                    f\"run_{run}_chunk_{current_chunk_iterator}.pkl\",\n",
    "                    verbose=verbose)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"--> Successfully saved WaveformSet of run {run}\")\n",
    "\n",
    "            # In this case, we need more waveforms for this run\n",
    "            elif len(aux_wfset.waveforms) < wvfs_left_to_read_for_this_run:\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"--> Didn't get enough waveforms from chunk \"\n",
    "                          f\"{current_chunk_iterator+1}/{len(rucio_filepaths)} \"\n",
    "                          f\"of run {run}\")\n",
    "                    print(f\"--> Expected {wvfs_left_to_read_for_this_run}, but only read {len(aux_wfset.waveforms)}\")\n",
    "\n",
    "                # In this case, try to read the same file but with a finer subsampling\n",
    "                if subsample > 1:\n",
    "                    # fReadSameChunkAgain is True by default\n",
    "                    subsample -= 1\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"--> Switching 'subsample' from {subsample+1} to {subsample} and reading it again...\")\n",
    "\n",
    "                # In this case, we read every waveform from this chunk, but we still\n",
    "                # haven't got enough waveforms, so go for the following chunk\n",
    "                else:\n",
    "                    subsample = subsample_seed\n",
    "                    fReadSameChunkAgain = False\n",
    "                    # fGoForAnotherChunk is True by default\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"--> All of the waveforms from this chunk were read\")\n",
    "                        print(f\"--> Saving them and proceeding to look for \"\n",
    "                              f\"{wvfs_left_to_read_for_this_run-len(aux_wfset.waveforms)} \"\n",
    "                              f\"(={wvfs_left_to_read_for_this_run}-{len(aux_wfset.waveforms)}) \"\n",
    "                              f\"waveforms from the following chunk ({current_chunk_iterator+2}/{len(rucio_filepaths)}) \"\n",
    "                              f\"of this run ({run}).\")\n",
    "\n",
    "                    dump_object_to_pickle(\n",
    "                        aux_wfset,\n",
    "                        saving_folderpath,\n",
    "                        f\"run_{run}_chunk_{current_chunk_iterator}.pkl\",\n",
    "                        verbose=verbose)\n",
    "\n",
    "                    # Switch to next chunk\n",
    "                    current_chunk_iterator += 1\n",
    "                    # But only read the waveforms that we need to add up to \n",
    "                    # average_wfs_per_channel * channels_no\n",
    "                    wvfs_left_to_read_for_this_run -= len(aux_wfset.waveforms)\n",
    "                    \n",
    "            # In this case, WaveformSet_from_hdf5_file() is misbehaving\n",
    "            else:\n",
    "                raise Exception(f\"WaveformSet_from_hdf5_file() is misbehaving. It read\"\n",
    "                                f\" more waveforms ({len(aux_wfset.waveforms)}) than \"\n",
    "                                f\"specified (wvfm_count={average_wfs_per_channel * channels_no})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_no = 3\n",
    "apa_no = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_folderpath = \"/afs/cern.ch/work/j/jurenago/private/repositories/waffles/src/waffles/np04_analysis/led_calibration/pickles\"\n",
    "average_wfs_per_channel = 3000\n",
    "channels_per_apa = 40\n",
    "# WARNING: Check that this parameter is correclty set for your particular case!\n",
    "acquired_apas = 1\n",
    "rucio_filepaths_folderpath = \"/eos/experiment/neutplatform/protodune/experiments/ProtoDUNE-II/PDS_Commissioning/waffles/1_rucio_paths/\"\n",
    "subsample_seed = 3\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data into pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pde in run_to_config[batch_no][apa_no].keys():\n",
    "\n",
    "    print(f\"--> Now retrieving data for PDE = {pde}\")\n",
    "\n",
    "    for run in run_to_config[batch_no][apa_no][pde].keys():\n",
    "\n",
    "        print(f\"\\t --> Now retrieving data for run = {run}\")\n",
    "        \n",
    "        aux = saving_folderpath + f\"/batch_{batch_no}/apa_{apa_no}/{pde}/\"\n",
    "\n",
    "        save_run_to_pickled_WaveformSet(\n",
    "            run,\n",
    "            aux,\n",
    "            average_wfs_per_channel=average_wfs_per_channel,\n",
    "            channels_no=channels_per_apa * acquired_apas,\n",
    "            rucio_filepaths_folderpath=rucio_filepaths_folderpath,\n",
    "            read_full_streaming_data=True if apa_no == 1 else False,\n",
    "            subsample_seed=subsample_seed,\n",
    "            verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(runs)):\n",
    "\n",
    "    aux = rucio_filepaths_folderpath+f\"/0{runs[i]}.txt\"\n",
    "\n",
    "    try:\n",
    "        rucio_filepaths = reader.get_filepaths_from_rucio(aux)\n",
    "    # Happens if there are no rucio filepaths for this run in rucio_filepaths_folderpath\n",
    "    except Exception:\n",
    "        print(f\"--> WARNING: Did not find the rucio paths for run {runs[i]}. Skipping this run.\")\n",
    "        continue\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"--> Processing run {runs[i]} ({i+1}/{len(runs)}): Found {len(rucio_filepaths)} chunks...\")\n",
    "\n",
    "    fGoForAnotherChunk = True\n",
    "    wvfs_left_to_read_for_this_run = average_wfs_per_channel * channels_no\n",
    "    current_chunk_iterator = 0\n",
    "\n",
    "    while fGoForAnotherChunk:\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\t --> Processing chunk {current_chunk_iterator+1}/{len(rucio_filepaths)} ...\")\n",
    "\n",
    "        subsample = subsample_seed\n",
    "        fReadSameChunkAgain = True\n",
    "\n",
    "        while fReadSameChunkAgain:\n",
    "\n",
    "            aux_wfset = reader.WaveformSet_from_hdf5_file( \n",
    "                rucio_filepaths[current_chunk_iterator],\n",
    "                read_full_streaming_data=read_full_streaming_data\n",
    "                subsample=subsample,\n",
    "                # WaveformSet_from_hdf5_file apparently subsamples from\n",
    "                # the [0, wvfm_count] range. Therefore, if we set\n",
    "                # wvfm_count to wvfs_left_to_read_for_this_run we\n",
    "                # will get, at most, wvfs_left_to_read_for_this_run/subsample\n",
    "                wvfm_count=wvfs_left_to_read_for_this_run*subsample,\n",
    "                )\n",
    "            \n",
    "            # In this case, we already have what we need for this run\n",
    "            if len(aux_wfset.waveforms) == wvfs_left_to_read_for_this_run:\n",
    "                fReadSameChunkAgain = False\n",
    "                fGoForAnotherChunk = False\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"--> Got enough waveforms ({len(aux_wfset.waveforms)}) \"\n",
    "                          f\"from chunk {current_chunk_iterator+1}/{len(rucio_filepaths)} \"\n",
    "                          f\"of run {runs[i]}\")\n",
    "                    print(f\"--> Now saving it to a pickle file ...\")\n",
    "\n",
    "                dump_object_to_pickle(\n",
    "                    aux_wfset,\n",
    "                    saving_folderpath+f\"{runs[i]}_chunk_{current_chunk_iterator}.pkl\")\n",
    "                \n",
    "                if verbose:\n",
    "                    try:\n",
    "                        print(f\"--> Switching to next run {runs[i+1]}\")\n",
    "                    # Happens if all runs were already processed\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "\n",
    "            # In this case, we need more waveforms for this run\n",
    "            elif len(aux_wfset.waveforms) < wvfs_left_to_read_for_this_run:\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"--> Didn't get enough waveforms from chunk \"\n",
    "                          f\"{current_chunk_iterator+1}/{len(rucio_filepaths)} \"\n",
    "                          f\"of run {runs[i]}\")\n",
    "                    print(f\"--> Expected {wvfs_left_to_read_for_this_run}, but only read {len(aux_wfset.waveforms)}\")\n",
    "\n",
    "                # In this case, try to read the same file but with a finer subsampling\n",
    "                if subsample > 1:\n",
    "                    # fReadSameChunkAgain is True by default\n",
    "                    subsample -= 1\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"--> Switching 'subsample' from {subsample+1} to {subsample} and reading it again...\")\n",
    "\n",
    "                # In this case, we read every waveform from this chunk, but we still\n",
    "                # haven't got enough waveforms, so go for the following chunk\n",
    "                else:\n",
    "                    subsample = subsample_seed\n",
    "                    fReadSameChunkAgain = False\n",
    "                    # fGoForAnotherChunk is True by default\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"--> All of the waveforms from this chunk were read\")\n",
    "                        print(f\"--> Saving them and proceeding to look for \"\n",
    "                              f\"{wvfs_left_to_read_for_this_run-len(aux_wfset.waveforms)} \"\n",
    "                              f\"(={wvfs_left_to_read_for_this_run}-{len(aux_wfset.waveforms)}) \"\n",
    "                              f\"waveforms from the following chunk ({current_chunk_iterator+2}/{len(rucio_filepaths)}) \"\n",
    "                              f\"of this run ({runs[i]}).\")\n",
    "\n",
    "                    dump_object_to_pickle(\n",
    "                        aux_wfset,\n",
    "                        saving_folderpath+f\"{runs[i]}_chunk_{current_chunk_iterator}.pkl\")\n",
    "\n",
    "                    # Switch to next chunk\n",
    "                    current_chunk_iterator += 1\n",
    "                    # But only read the waveforms that we need to add up to \n",
    "                    # average_wfs_per_channel * channels_no\n",
    "                    wvfs_left_to_read_for_this_run -= len(aux_wfset.waveforms)\n",
    "                    \n",
    "            # In this case, WaveformSet_from_hdf5_file() is misbehaving\n",
    "            else:\n",
    "                raise Exception(f\"WaveformSet_from_hdf5_file() read more waveforms\"\n",
    "                                f\" ({len(aux_wfset.waveforms)}) than specified (wvfm_count=\"\n",
    "                                f\"{average_wfs_per_channel * channels_no})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
