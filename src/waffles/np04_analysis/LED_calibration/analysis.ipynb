{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import plotly.subplots as psu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from waffles.data_classes.Waveform import Waveform\n",
    "from waffles.data_classes.WaveformSet import WaveformSet\n",
    "from waffles.data_classes.ChannelWSGrid import ChannelWSGrid\n",
    "from waffles.data_classes.IPDict import IPDict\n",
    "from waffles.data_classes.BasicWfAna import BasicWfAna\n",
    "from waffles.input.raw_ROOT_reader import WaveformSet_from_ROOT_files\n",
    "from waffles.utils.fit_peaks.fit_peaks import fit_peaks_of_ChannelWSGrid\n",
    "from waffles.plotting.plot import plot_ChannelWSGrid\n",
    "from waffles.np04_utils.utils import get_channel_iterator\n",
    "from waffles.np04_analysis.LED_calibration.LED_configuration_to_channel import config_to_channels\n",
    "from waffles.np04_analysis.LED_calibration.run_number_to_LED_configuration import run_to_config\n",
    "from waffles.np04_analysis.LED_calibration.excluded_channels import excluded_channels\n",
    "from waffles.np04_data.ProtoDUNE_HD_APA_maps import APA_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_folderpath(run, base_folderpath):\n",
    "    return f\"{base_folderpath}/run_0{run}\"\n",
    "\n",
    "def comes_from_channel( waveform : Waveform, \n",
    "                        endpoint, \n",
    "                        channels) -> bool:\n",
    "\n",
    "    if waveform.Endpoint == endpoint:\n",
    "        if waveform.Channel in channels:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_batch = 1  # 1, ...\n",
    "apa_no = 4  # 1, 2, 3, 4\n",
    "pde = 0.45  # 0.40, 0.45, 0.50\n",
    "base_folderpath = '/eos/experiment/neutplatform/protodune/experiments/ProtoDUNE-II/PDS_Commissioning/waffles/2_daq_root'\n",
    "plots_saving_filepath = \"\"\n",
    "path_to_output_summary_dataframe = os.path.join(os.getcwd(), 'calibration_batches/batch_1/output.pkl')\n",
    "\n",
    "hpk_ov = {0.4 : 2.0, 0.45 : 3.5, 0.50 : 4.0}[pde]\n",
    "fbk_ov = {0.4 : 3.5, 0.45 : 4.5, 0.50 : 7.0}[pde]\n",
    "ov_no = {0.4 : 1, 0.45 : 2, 0.50 : 3}[pde]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_to_config_ = run_to_config[measurements_batch][apa_no][pde]\n",
    "config_to_channels_ = config_to_channels[apa_no][pde]\n",
    "excluded_channels_ = excluded_channels[measurements_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the analysis input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_label = 'standard'\n",
    "\n",
    "if apa_no == 1:\n",
    "\n",
    "    starting_tick = {   27818 : 621,\n",
    "                        27820 : 615,\n",
    "                        27822 : 615,\n",
    "                        27823 : 615,\n",
    "                        27824 : 615,\n",
    "                        27825 : 615,\n",
    "                        27826 : 615,\n",
    "                        27827 : 632,\n",
    "                        27828 : 626,\n",
    "                        27898 : 635,\n",
    "                        27899 : 635,\n",
    "                        27900 : 618,\n",
    "                        27921 : 602,\n",
    "                        27901 : 615,\n",
    "                        27902 : 615,\n",
    "                        27903 : 615,\n",
    "                        27904 : 630,\n",
    "                        27905 : 620,\n",
    "                        27906 : 610,\n",
    "                        27907 : 608,\n",
    "                        27908 : 602}\n",
    "    \n",
    "    baseline_limits = [100, 400]\n",
    "\n",
    "else:\n",
    "\n",
    "    starting_tick = { run : 125 for run in run_to_config_.keys() }\n",
    "    baseline_limits = [0, 100, 900, 1000]\n",
    "\n",
    "aux_width = 40  # Integration window width\n",
    "\n",
    "input_parameters = IPDict(baseline_limits = baseline_limits)\n",
    "checks_kwargs = IPDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and analyse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_run = list(run_to_config_.keys())[0]\n",
    "first_endpoint = list(config_to_channels_[run_to_config_[first_run]].keys())[0]\n",
    "channels = config_to_channels_[run_to_config_[first_run]][first_endpoint]\n",
    "\n",
    "stop_fraction = 1.0\n",
    "wfset = WaveformSet_from_ROOT_files('pyroot',\n",
    "                                    folderpath = get_run_folderpath(first_run, base_folderpath),\n",
    "                                    bulk_data_tree_name = 'raw_waveforms', \n",
    "                                    meta_data_tree_name = 'metadata',\n",
    "                                    set_offset_wrt_daq_window = True if apa_no == 1 else False,\n",
    "                                    read_full_streaming_data = True if apa_no == 1 else False,\n",
    "                                    truncate_wfs_to_minimum = True if apa_no == 1 else False,\n",
    "                                    start_fraction = 0.0,\n",
    "                                    stop_fraction = stop_fraction,\n",
    "                                    subsample = 1,\n",
    "                                    verbose = True)\n",
    "\n",
    "wfset = WaveformSet.from_filtered_WaveformSet(  wfset,\n",
    "                                                comes_from_channel,\n",
    "                                                first_endpoint,\n",
    "                                                channels)\n",
    "\n",
    "input_parameters['int_ll'] = starting_tick[first_run]\n",
    "input_parameters['int_ul'] = starting_tick[first_run] + aux_width\n",
    "input_parameters['amp_ll'] = starting_tick[first_run]\n",
    "input_parameters['amp_ul'] = starting_tick[first_run] + aux_width\n",
    "checks_kwargs['points_no'] = wfset.PointsPerWf\n",
    "\n",
    "_ = wfset.analyse(  analysis_label,\n",
    "                    BasicWfAna,\n",
    "                    input_parameters,\n",
    "                    *[], # *args,\n",
    "                    analysis_kwargs = {},\n",
    "                    checks_kwargs = checks_kwargs,\n",
    "                    overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in run_to_config_.keys():\n",
    "    for endpoint in config_to_channels_[run_to_config_[run]].keys():\n",
    "        if run == first_run and endpoint == first_endpoint:\n",
    "            continue\n",
    "\n",
    "        new_wfset = WaveformSet_from_ROOT_files('pyroot',\n",
    "                                                folderpath = get_run_folderpath(run, base_folderpath),\n",
    "                                                bulk_data_tree_name = 'raw_waveforms', \n",
    "                                                meta_data_tree_name = 'metadata',\n",
    "                                                set_offset_wrt_daq_window = True if apa_no == 1 else False,\n",
    "                                                read_full_streaming_data = True if apa_no == 1 else False,\n",
    "                                                truncate_wfs_to_minimum = True if apa_no == 1 else False,\n",
    "                                                start_fraction = 0.0,\n",
    "                                                stop_fraction = stop_fraction,\n",
    "                                                subsample = 1)\n",
    "                \n",
    "        new_wfset = WaveformSet.from_filtered_WaveformSet(  new_wfset,\n",
    "                                                            comes_from_channel,\n",
    "                                                            endpoint,\n",
    "                                                            config_to_channels_[run_to_config_[run]][endpoint])\n",
    "        input_parameters['int_ll'] = starting_tick[run]\n",
    "        input_parameters['int_ul'] = starting_tick[run] + aux_width\n",
    "        input_parameters['amp_ll'] = starting_tick[run]\n",
    "        input_parameters['amp_ul'] = starting_tick[run] + aux_width\n",
    "        checks_kwargs['points_no'] = new_wfset.PointsPerWf\n",
    "\n",
    "        print(\"\\n Now analysing waveforms from:\")\n",
    "        print(f\" - run {run}\")\n",
    "        print(f\" - endpoint {endpoint}\")\n",
    "        print(f\" - channels {config_to_channels_[run_to_config_[run]][endpoint]} \\n\")      \n",
    "\n",
    "        _ = new_wfset.analyse(  analysis_label,\n",
    "                                BasicWfAna,\n",
    "                                input_parameters,\n",
    "                                *[], # *args,\n",
    "                                analysis_kwargs = {},\n",
    "                                checks_kwargs = checks_kwargs,\n",
    "                                overwrite = True)\n",
    "        wfset.merge(new_wfset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the read channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfset.AvailableChannels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the fitting input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_peaks = 2                               # Maximum number of peaks to fit\n",
    "\n",
    "prominence = 0.15           # [0.15 - 0.2]  # Minimal prominence, as a fraction of the y-range, for a peak to be detected\n",
    "\n",
    "half_points_to_fit = 2      # [2 - 3]       # The number of points to fit on either side of the peak maximum\n",
    "                                            # P.e. setting this to 2 will fit 5 points in total: the maximum and 2 points on either side\n",
    "\n",
    "bins_number = 125           # [125 - 150]   # Number of bins for the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_apa = ChannelWSGrid(   APA_map[apa_no],\n",
    "                            wfset,\n",
    "                            compute_calib_histo = True,\n",
    "                            bins_number = bins_number,                       \n",
    "                            domain = np.array((-10000., 50000.)),\n",
    "                            variable = 'integral',\n",
    "                            analysis_label = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_peaks_of_ChannelWSGrid( grid_apa,\n",
    "                            max_peaks,\n",
    "                            prominence,\n",
    "                            half_points_to_fit,\n",
    "                            initial_percentage = 0.15,\n",
    "                            percentage_step = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f\"APA {apa_no} - Runs {list(wfset.Runs)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot calibration histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = psu.make_subplots( rows = 10,\n",
    "                            cols = 4)\n",
    "\n",
    "plot_ChannelWSGrid( grid_apa,\n",
    "                    figure = figure,\n",
    "                    share_x_scale = False,\n",
    "                    share_y_scale = False,\n",
    "                    mode = 'calibration',\n",
    "                    wfs_per_axes = None,\n",
    "                    analysis_label = analysis_label,\n",
    "                    plot_peaks_fits = True,\n",
    "                    detailed_label = False,\n",
    "                    verbose = True)\n",
    "\n",
    "figure.update_layout(   title = {\n",
    "                                    'text': title,\n",
    "                                    'font': {\n",
    "                                                'size': 24  # Specify the font size for the title\n",
    "                                            }\n",
    "                                },\n",
    "                        width = 1100,\n",
    "                        height = 1200,\n",
    "                        showlegend = True)\n",
    "\n",
    "figure.show()\n",
    "\n",
    "# figure.write_image(f\"{plots_saving_filepath}/apa_{apa_no}_calibration_histograms.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the fit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for i in range(grid_apa.ChMap.Rows):\n",
    "    for j in range(grid_apa.ChMap.Columns):\n",
    "\n",
    "        if grid_apa.ChMap.Data[i][j].Endpoint in excluded_channels_[apa_no][pde].keys():\n",
    "            if grid_apa.ChMap.Data[i][j].Channel in excluded_channels_[apa_no][pde][grid_apa.ChMap.Data[i][j].Endpoint]:\n",
    "                print(f\"Excluding channel {grid_apa.ChMap.Data[i][j].Channel} from endpoint {grid_apa.ChMap.Data[i][j].Endpoint}...\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            fit_params = grid_apa.ChWfSets[grid_apa.ChMap.Data[i][j].Endpoint][grid_apa.ChMap.Data[i][j].Channel].CalibHisto.GaussianFitsParameters\n",
    "        except KeyError:\n",
    "            print(f\"Endpoint {grid_apa.ChMap.Data[i][j].Endpoint}, channel {grid_apa.ChMap.Data[i][j].Channel} not found in data. Continuing...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            aux = data[grid_apa.ChMap.Data[i][j].Endpoint]\n",
    "        except KeyError:\n",
    "            data[grid_apa.ChMap.Data[i][j].Endpoint] = {}\n",
    "            aux = data[grid_apa.ChMap.Data[i][j].Endpoint]\n",
    "    \n",
    "        try:\n",
    "            aux_gain = fit_params['mean'][1][0] - fit_params['mean'][0][0]\n",
    "        except IndexError:\n",
    "            print(f\"Endpoint {grid_apa.ChMap.Data[i][j].Endpoint}, channel {grid_apa.ChMap.Data[i][j].Channel} not found in data. Continuing...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            aux_2 = aux[grid_apa.ChMap.Data[i][j].Channel]\n",
    "        except KeyError:\n",
    "            aux[grid_apa.ChMap.Data[i][j].Channel] = {}\n",
    "            aux_2 = aux[grid_apa.ChMap.Data[i][j].Channel]\n",
    "\n",
    "        aux_2['gain'] = aux_gain\n",
    "        aux_2['snr'] = aux_gain/np.sqrt( fit_params['std'][0][0]**2 + fit_params['std'][1][0]**2 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the fit data to a running dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actually_save = False   # Warning: Settings this variable to True will save\n",
    "                        # changes to the output dataframe, potentially introducing\n",
    "                        # spurious data. Only set it to True if you are sure of what\n",
    "                        # you are saving.\n",
    "\n",
    "overwrite = True # Do you want to potentially overwrite existing rows of the dataframe?\n",
    "\n",
    "expected_columns = {'APA' : [], \n",
    "                    'endpoint' : [], \n",
    "                    'channel' : [], \n",
    "                    'channel_iterator' : [], \n",
    "                    'PDE' : [], \n",
    "                    'gain' : [] , \n",
    "                    'snr' : [], \n",
    "                    'OV#' : [], \n",
    "                    'HPK_OV_V' : [], \n",
    "                    'FBK_OV_V' : [] }\n",
    "\n",
    "if not os.path.exists(path_to_output_summary_dataframe):                        # If the file does not exist, create it\n",
    "\n",
    "    df = pd.DataFrame(expected_columns)\n",
    "\n",
    "    df['APA'] = df['APA'].astype(int)               # Force column-wise types\n",
    "    df['endpoint'] = df['endpoint'].astype(int)\n",
    "    df['channel'] = df['channel'].astype(int)\n",
    "    df['channel_iterator'] = df['channel_iterator'].astype(int)\n",
    "    df['PDE'] = df['PDE'].astype(float)\n",
    "    df['gain'] = df['gain'].astype(float)\n",
    "    df['snr'] = df['snr'].astype(float)\n",
    "    df['OV#'] = df['OV#'].astype(int)\n",
    "    df['HPK_OV_V'] = df['HPK_OV_V'].astype(float)\n",
    "    df['FBK_OV_V'] = df['FBK_OV_V'].astype(float)\n",
    "\n",
    "    df.to_pickle(path_to_output_summary_dataframe)\n",
    "\n",
    "df = pd.read_pickle(path_to_output_summary_dataframe)\n",
    "\n",
    "if len(df.columns) != len(expected_columns):\n",
    "    raise Exception(f\"The columns of the found dataframe do not match the expected ones. Something went wrong.\")\n",
    "\n",
    "elif not bool(np.prod(df.columns == pd.Index(data = expected_columns))):\n",
    "    raise Exception(f\"The columns of the found dataframe do not match the expected ones. Something went wrong.\")\n",
    "\n",
    "else:\n",
    "    for endpoint in data.keys():\n",
    "        for channel in data[endpoint]:\n",
    "\n",
    "            new_row = { 'APA' : [int(apa_no)],             # Assemble the new row\n",
    "                        'endpoint' : [endpoint],\n",
    "                        'channel' : [channel],\n",
    "                        'channel_iterator' : [get_channel_iterator(apa_no, endpoint, channel)],\n",
    "                        'PDE' : [pde],\n",
    "                        'gain' : [data[endpoint][channel]['gain']],\n",
    "                        'snr' : [data[endpoint][channel]['snr']],\n",
    "                        'OV#' : [ov_no],\n",
    "                        'HPK_OV_V' : [hpk_ov],\n",
    "                        'FBK_OV_V' : [fbk_ov] }\n",
    "\n",
    "            matching_rows_indices = df[(df['endpoint'] == endpoint) &       # Check if there is already an entry for the\n",
    "                                       (df['channel'] == channel) &         # given endpoint and channel for this OV\n",
    "                                       (df['OV#'] == ov_no)].index          \n",
    "                                                                            \n",
    "            if len(matching_rows_indices) > 1:\n",
    "                raise Exception(f\"There are already more than one rows for the given endpoint ({endpoint}), channel ({channel}) and OV# ({ov_no}). Something went wrong.\")\n",
    "            \n",
    "            elif len(matching_rows_indices) == 1:\n",
    "                if overwrite:\n",
    "\n",
    "                    row_index = matching_rows_indices[0]\n",
    "\n",
    "                    new_row = { key : new_row[key][0] for key in new_row.keys() }  \n",
    "\n",
    "                    if actually_save:\n",
    "                        df.loc[row_index, :] = new_row\n",
    "                \n",
    "                else:\n",
    "                    print(f\"Skipping the entry for endpoint {endpoint}, channel {channel} and OV# {ov_no} ...\")\n",
    "\n",
    "            else:   # len(matching_rows_indices) == 0\n",
    "            \n",
    "                if actually_save:\n",
    "                    df = pd.concat([df, pd.DataFrame(new_row)], axis = 0, ignore_index = True)\n",
    "                    df.reset_index()\n",
    "\n",
    "    df.to_pickle(path_to_output_summary_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
