{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import plotly.subplots as psu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from waffles.data_classes.Waveform import Waveform\n",
    "from waffles.data_classes.WaveformSet import WaveformSet\n",
    "from waffles.data_classes.ChannelWsGrid import ChannelWsGrid\n",
    "from waffles.data_classes.IPDict import IPDict\n",
    "from waffles.data_classes.BasicWfAna import BasicWfAna\n",
    "from waffles.input.raw_root_reader import WaveformSet_from_root_files\n",
    "from waffles.input.pickle_file_reader import WaveformSet_from_pickle_files\n",
    "from waffles.utils.fit_peaks.fit_peaks import fit_peaks_of_ChannelWsGrid\n",
    "from waffles.plotting.plot import plot_ChannelWsGrid\n",
    "from waffles.np04_utils.utils import get_channel_iterator\n",
    "from waffles.np04_analysis.LED_calibration.calibration_batches.LED_configuration_to_channel import config_to_channels\n",
    "from waffles.np04_analysis.LED_calibration.calibration_batches.run_number_to_LED_configuration import run_to_config\n",
    "from waffles.np04_analysis.LED_calibration.calibration_batches.excluded_channels import excluded_channels\n",
    "from waffles.np04_data.ProtoDUNE_HD_APA_maps import APA_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_folderpath(run, base_folderpath):\n",
    "    return f\"{base_folderpath}/data/run_0{run}\"\n",
    "\n",
    "def get_apa_foldername(measurements_batch, apa_no):\n",
    "    \"\"\"This function encapsulates the non-homogeneous \n",
    "    naming convention of the APA folders depending \n",
    "    on the measurements batch.\"\"\" \n",
    "\n",
    "    if measurements_batch not in [1, 2, 3]:\n",
    "        raise ValueError(f\"Measurements batch {measurements_batch} is not valid\")\n",
    "    \n",
    "    if apa_no not in [1, 2, 3, 4]:\n",
    "        raise ValueError(f\"APA number {apa_no} is not valid\")\n",
    "                         \n",
    "    if measurements_batch == 1:\n",
    "        if apa_no in [1, 2]:\n",
    "            return 'apas_12'\n",
    "        else:\n",
    "            return 'apas_34'\n",
    "        \n",
    "    if measurements_batch in [2, 3]:\n",
    "        if apa_no == 1:\n",
    "            return 'apa_1'\n",
    "        elif apa_no == 2:\n",
    "            return 'apa_2'\n",
    "        else:\n",
    "            return 'apas_34'\n",
    "\n",
    "def comes_from_channel(\n",
    "        waveform: Waveform, \n",
    "        endpoint, \n",
    "        channels) -> bool:\n",
    "\n",
    "    if waveform.endpoint == endpoint:\n",
    "        if waveform.channel in channels:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where to look for the root files or the pickle files\n",
    "data_folderpath = ''\n",
    "# Path where to save the plots\n",
    "plots_saving_filepath = ''\n",
    "\n",
    "measurements_batch = 1  # 1, ...\n",
    "fProcessRootNotPickles = True if measurements_batch == 1 else False\n",
    "apa_no = 4  # 1, 2, 3, 4\n",
    "pde = 0.45  # 0.40, 0.45, 0.50\n",
    "path_to_output_summary_dataframe = os.path.join(\n",
    "    os.getcwd(), \n",
    "    f\"calibration_batches/batch_{measurements_batch}/output.pkl\")\n",
    "\n",
    "hpk_ov = {0.4 : 2.0, 0.45 : 3.5, 0.50 : 4.0}[pde]\n",
    "fbk_ov = {0.4 : 3.5, 0.45 : 4.5, 0.50 : 7.0}[pde]\n",
    "ov_no = {0.4 : 1, 0.45 : 2, 0.50 : 3}[pde]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_to_config_ = run_to_config[measurements_batch][apa_no][pde]\n",
    "config_to_channels_ = config_to_channels[measurements_batch][apa_no][pde]\n",
    "excluded_channels_ = excluded_channels[measurements_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the analysis input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_label = 'standard'\n",
    "\n",
    "if apa_no == 1:\n",
    "\n",
    "    starting_tick = {\n",
    "        27818: 621,\n",
    "        27820: 615,\n",
    "        27822: 615,\n",
    "        27823: 615,\n",
    "        27824: 615,\n",
    "        27825: 615,\n",
    "        27826: 615,\n",
    "        27827: 632,\n",
    "        27828: 626,\n",
    "        27898: 635,\n",
    "        27899: 635,\n",
    "        27900: 618,\n",
    "        27921: 602,\n",
    "        27901: 615,\n",
    "        27902: 615,\n",
    "        27903: 615,\n",
    "        27904: 630,\n",
    "        27905: 620,\n",
    "        27906: 610,\n",
    "        27907: 608,\n",
    "        27908: 602\n",
    "    }\n",
    "    \n",
    "    baseline_limits = [100, 400]\n",
    "\n",
    "else:\n",
    "\n",
    "    starting_tick = {\n",
    "        run: 125 for run in run_to_config_.keys()\n",
    "    }\n",
    "    baseline_limits = [0, 100, 900, 1000]\n",
    "\n",
    "# Integration window width\n",
    "aux_width = 40\n",
    "\n",
    "input_parameters = IPDict(baseline_limits=baseline_limits)\n",
    "checks_kwargs = IPDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and analyse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_run = list(run_to_config_.keys())[0]\n",
    "first_endpoint = list(config_to_channels_[run_to_config_[first_run]].keys())[0]\n",
    "channels = config_to_channels_[run_to_config_[first_run]][first_endpoint]\n",
    "\n",
    "if fProcessRootNotPickles:\n",
    "\n",
    "    stop_fraction = 1.0\n",
    "    wfset = WaveformSet_from_root_files(\n",
    "        \"pyroot\",\n",
    "        folderpath=get_run_folderpath(first_run, data_folderpath),\n",
    "        bulk_data_tree_name=\"raw_waveforms\",\n",
    "        meta_data_tree_name=\"metadata\",\n",
    "        set_offset_wrt_daq_window=True if apa_no == 1 else False,\n",
    "        read_full_streaming_data=True if apa_no == 1 else False,\n",
    "        truncate_wfs_to_minimum=True if apa_no == 1 else False,\n",
    "        start_fraction=0.0,\n",
    "        stop_fraction=stop_fraction,\n",
    "        subsample=1,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "else:\n",
    "\n",
    "    wfset = WaveformSet_from_pickle_files(\n",
    "        folderpath=get_run_folderpath(first_run, data_folderpath),\n",
    "        target_extension=\".pkl\",\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfset = WaveformSet.from_filtered_WaveformSet(\n",
    "    wfset, \n",
    "    comes_from_channel, \n",
    "    first_endpoint, \n",
    "    channels\n",
    ")\n",
    "\n",
    "input_parameters['int_ll'] = starting_tick[first_run]\n",
    "input_parameters['int_ul'] = starting_tick[first_run] + aux_width\n",
    "input_parameters['amp_ll'] = starting_tick[first_run]\n",
    "input_parameters['amp_ul'] = starting_tick[first_run] + aux_width\n",
    "checks_kwargs['points_no'] = wfset.points_per_wf\n",
    "\n",
    "_ = wfset.analyse(\n",
    "    analysis_label,\n",
    "    BasicWfAna,\n",
    "    input_parameters,\n",
    "    *[],  # *args,\n",
    "    analysis_kwargs={},\n",
    "    checks_kwargs=checks_kwargs,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in run_to_config_.keys():\n",
    "    for endpoint in config_to_channels_[run_to_config_[run]].keys():\n",
    "        if run == first_run and endpoint == first_endpoint:\n",
    "            continue\n",
    "\n",
    "        if fProcessRootNotPickles:\n",
    "            new_wfset = WaveformSet_from_root_files(\n",
    "                \"pyroot\",\n",
    "                folderpath=get_run_folderpath(run, data_folderpath),\n",
    "                bulk_data_tree_name=\"raw_waveforms\",\n",
    "                meta_data_tree_name=\"metadata\",\n",
    "                set_offset_wrt_daq_window=True if apa_no == 1 else False,\n",
    "                read_full_streaming_data=True if apa_no == 1 else False,\n",
    "                truncate_wfs_to_minimum=True if apa_no == 1 else False,\n",
    "                start_fraction=0.0,\n",
    "                stop_fraction=stop_fraction,\n",
    "                subsample=1,\n",
    "            )\n",
    "        else:\n",
    "            new_wfset = WaveformSet_from_pickle_files(\n",
    "                folderpath=get_run_folderpath(run, data_folderpath),\n",
    "                target_extension=\".pkl\",\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "        new_wfset = WaveformSet.from_filtered_WaveformSet(\n",
    "            new_wfset,\n",
    "            comes_from_channel,\n",
    "            endpoint,\n",
    "            config_to_channels_[run_to_config_[run]][endpoint],\n",
    "        )\n",
    "\n",
    "        input_parameters['int_ll'] = starting_tick[run]\n",
    "        input_parameters['int_ul'] = starting_tick[run] + aux_width\n",
    "        input_parameters['amp_ll'] = starting_tick[run]\n",
    "        input_parameters['amp_ul'] = starting_tick[run] + aux_width\n",
    "        checks_kwargs['points_no'] = new_wfset.points_per_wf\n",
    "\n",
    "        print(\"\\n Now analysing waveforms from:\")\n",
    "        print(f\" - run {run}\")\n",
    "        print(f\" - endpoint {endpoint}\")\n",
    "        print(f\" - channels {config_to_channels_[run_to_config_[run]][endpoint]} \\n\")      \n",
    "\n",
    "        _ = new_wfset.analyse(\n",
    "            analysis_label,\n",
    "            BasicWfAna,\n",
    "            input_parameters,\n",
    "            *[],  # *args,\n",
    "            analysis_kwargs={},\n",
    "            checks_kwargs=checks_kwargs,\n",
    "            overwrite=True,\n",
    "        )\n",
    "        \n",
    "        wfset.merge(new_wfset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the read channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfset.available_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the fitting input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of peaks to fit\n",
    "max_peaks = 2\n",
    "\n",
    "# Minimal prominence, as a fraction of the y-range, for a peak to be detected\n",
    "prominence = 0.15 #Â [0.10 - 0.2]\n",
    "\n",
    "# The number of points to fit on either side of the peak maximum\n",
    "# P.e. setting this to 2 will fit 5 points in total: the maximum and 2 points on either side\n",
    "half_points_to_fit = 2 # [2 - 3]       \n",
    "\n",
    "# Number of bins for the histogram\n",
    "bins_number = 125 # [90 - 125]\n",
    "\n",
    "if apa_no in [2, 3, 4]:\n",
    "    if pde == 0.4:\n",
    "        bins_number = 125\n",
    "    elif pde == 0.45:\n",
    "        bins_number = 110 # [100-110]\n",
    "    else:\n",
    "        bins_number = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_apa = ChannelWsGrid(\n",
    "    APA_map[apa_no],\n",
    "    wfset,\n",
    "    compute_calib_histo=True,\n",
    "    bins_number=bins_number,\n",
    "    domain=np.array((-10000.0, 50000.0)),\n",
    "    variable=\"integral\",\n",
    "    analysis_label=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_peaks_of_ChannelWsGrid(\n",
    "    grid_apa,\n",
    "    max_peaks,\n",
    "    prominence,\n",
    "    half_points_to_fit,\n",
    "    initial_percentage=0.15,\n",
    "    percentage_step=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f\"APA {apa_no} - Runs {list(wfset.runs)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot calibration histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = psu.make_subplots(\n",
    "    rows=10, \n",
    "    cols=4\n",
    ")\n",
    "\n",
    "plot_ChannelWsGrid(\n",
    "    grid_apa,\n",
    "    figure=figure,\n",
    "    share_x_scale=False,\n",
    "    share_y_scale=False,\n",
    "    mode=\"calibration\",\n",
    "    wfs_per_axes=None,\n",
    "    analysis_label=analysis_label,\n",
    "    plot_peaks_fits=True,\n",
    "    detailed_label=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "figure.update_layout(\n",
    "    title={\n",
    "        \"text\": title,\n",
    "        \"font\": {\"size\": 24}\n",
    "    },\n",
    "    width=1100,\n",
    "    height=1200,\n",
    "    showlegend=True,\n",
    ")\n",
    "\n",
    "figure.show()\n",
    "\n",
    "# figure.write_image(f\"{plots_saving_filepath}/apa_{apa_no}_calibration_histograms.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the fit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for i in range(grid_apa.ch_map.rows):\n",
    "    for j in range(grid_apa.ch_map.columns):\n",
    "\n",
    "        if grid_apa.ch_map.data[i][j].endpoint in excluded_channels_[apa_no][pde].keys():\n",
    "            if grid_apa.ch_map.data[i][j].channel in excluded_channels_[apa_no][pde][grid_apa.ch_map.data[i][j].endpoint]:\n",
    "                print(f\"Excluding channel {grid_apa.ch_map.data[i][j].channel} from endpoint {grid_apa.ch_map.data[i][j].endpoint}...\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            fit_params = grid_apa.ch_wf_sets[grid_apa.ch_map.data[i][j].endpoint][grid_apa.ch_map.data[i][j].channel].calib_histo.gaussian_fits_parameters\n",
    "        except KeyError:\n",
    "            print(f\"Endpoint {grid_apa.ch_map.data[i][j].endpoint}, channel {grid_apa.ch_map.data[i][j].channel} not found in data. Continuing...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            aux = data[grid_apa.ch_map.data[i][j].endpoint]\n",
    "        except KeyError:\n",
    "            data[grid_apa.ch_map.data[i][j].endpoint] = {}\n",
    "            aux = data[grid_apa.ch_map.data[i][j].endpoint]\n",
    "    \n",
    "        try:\n",
    "            aux_gain = fit_params['mean'][1][0] - fit_params['mean'][0][0]\n",
    "        except IndexError:\n",
    "            print(f\"Endpoint {grid_apa.ch_map.data[i][j].endpoint}, channel {grid_apa.ch_map.data[i][j].channel} not found in data. Continuing...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            aux_2 = aux[grid_apa.ch_map.data[i][j].channel]\n",
    "        except KeyError:\n",
    "            aux[grid_apa.ch_map.data[i][j].channel] = {}\n",
    "            aux_2 = aux[grid_apa.ch_map.data[i][j].channel]\n",
    "\n",
    "        aux_2['gain'] = aux_gain\n",
    "        aux_2['snr'] = aux_gain/np.sqrt( fit_params['std'][0][0]**2 + fit_params['std'][1][0]**2 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the fit data to a running dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: Settings this variable to True will save\n",
    "# changes to the output dataframe, potentially introducing\n",
    "# spurious data. Only set it to True if you are sure of what\n",
    "# you are saving.\n",
    "actually_save = False   \n",
    "\n",
    "# Do you want to potentially overwrite existing rows of the dataframe?\n",
    "overwrite = True\n",
    "\n",
    "expected_columns = {\n",
    "    \"APA\": [],\n",
    "    \"endpoint\": [],\n",
    "    \"channel\": [],\n",
    "    \"channel_iterator\": [],\n",
    "    \"PDE\": [],\n",
    "    \"gain\": [],\n",
    "    \"snr\": [],\n",
    "    \"OV#\": [],\n",
    "    \"HPK_OV_V\": [],\n",
    "    \"FBK_OV_V\": [],\n",
    "}\n",
    "\n",
    "# If the file does not exist, create it\n",
    "if not os.path.exists(path_to_output_summary_dataframe):\n",
    "\n",
    "    df = pd.DataFrame(expected_columns)\n",
    "\n",
    "    # Force column-wise types\n",
    "    df['APA'] = df['APA'].astype(int)\n",
    "    df['endpoint'] = df['endpoint'].astype(int)\n",
    "    df['channel'] = df['channel'].astype(int)\n",
    "    df['channel_iterator'] = df['channel_iterator'].astype(int)\n",
    "    df['PDE'] = df['PDE'].astype(float)\n",
    "    df['gain'] = df['gain'].astype(float)\n",
    "    df['snr'] = df['snr'].astype(float)\n",
    "    df['OV#'] = df['OV#'].astype(int)\n",
    "    df['HPK_OV_V'] = df['HPK_OV_V'].astype(float)\n",
    "    df['FBK_OV_V'] = df['FBK_OV_V'].astype(float)\n",
    "\n",
    "    df.to_pickle(path_to_output_summary_dataframe)\n",
    "\n",
    "df = pd.read_pickle(path_to_output_summary_dataframe)\n",
    "\n",
    "if len(df.columns) != len(expected_columns):\n",
    "    raise Exception(f\"The columns of the found dataframe do not match the expected ones. Something went wrong.\")\n",
    "\n",
    "elif not bool(np.prod(df.columns == pd.Index(data = expected_columns))):\n",
    "    raise Exception(f\"The columns of the found dataframe do not match the expected ones. Something went wrong.\")\n",
    "\n",
    "else:\n",
    "    for endpoint in data.keys():\n",
    "        for channel in data[endpoint]:\n",
    "\n",
    "            # Assemble the new row\n",
    "            new_row = {\n",
    "                \"APA\": [int(apa_no)],\n",
    "                \"endpoint\": [endpoint],\n",
    "                \"channel\": [channel],\n",
    "                \"channel_iterator\": [get_channel_iterator(apa_no, endpoint, channel)],\n",
    "                \"PDE\": [pde],\n",
    "                \"gain\": [data[endpoint][channel][\"gain\"]],\n",
    "                \"snr\": [data[endpoint][channel][\"snr\"]],\n",
    "                \"OV#\": [ov_no],\n",
    "                \"HPK_OV_V\": [hpk_ov],\n",
    "                \"FBK_OV_V\": [fbk_ov],\n",
    "            }\n",
    "\n",
    "            # Check if there is already an entry for the\n",
    "            # given endpoint and channel for this OV\n",
    "            matching_rows_indices = df[\n",
    "                (df['endpoint'] == endpoint) &       \n",
    "                (df['channel'] == channel) &\n",
    "                (df['OV#'] == ov_no)].index          \n",
    "\n",
    "            if len(matching_rows_indices) > 1:\n",
    "                raise Exception(f\"There are already more than one rows for the given endpoint ({endpoint}), channel ({channel}) and OV# ({ov_no}). Something went wrong.\")\n",
    "\n",
    "            elif len(matching_rows_indices) == 1:\n",
    "                if overwrite:\n",
    "\n",
    "                    row_index = matching_rows_indices[0]\n",
    "\n",
    "                    new_row = { key : new_row[key][0] for key in new_row.keys() }  \n",
    "\n",
    "                    if actually_save:\n",
    "                        df.loc[row_index, :] = new_row\n",
    "\n",
    "                else:\n",
    "                    print(f\"Skipping the entry for endpoint {endpoint}, channel {channel} and OV# {ov_no} ...\")\n",
    "\n",
    "            else: # len(matching_rows_indices) == 0\n",
    "\n",
    "                if actually_save:\n",
    "                    df = pd.concat([df, pd.DataFrame(new_row)], axis = 0, ignore_index = True)\n",
    "                    df.reset_index()\n",
    "\n",
    "    df.to_pickle(path_to_output_summary_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
